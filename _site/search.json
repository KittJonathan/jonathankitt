[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "jonathankitt",
    "section": "",
    "text": "TidyTuesday 2023 - Week 31\n\n\nU.S. states\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ndatavis\n\n\nU.S.\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nJonathan Kitt\n\n\n\n\n\n\n  \n\n\n\n\nTidyTuesday 2023 - Week 30\n\n\nCuring scurvy\n\n\n\n\nR\n\n\nTidyTuesday\n\n\ndatavis\n\n\nscurvy\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nJonathan Kitt\n\n\n\n\n\n\n  \n\n\n\n\nThe {datardis} package\n\n\nExplore the Dr Who TV series\n\n\n\n\nR\n\n\npackage\n\n\nDr Who\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nJonathan Kitt\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hi, I am Jonathan üëã\n\nI currently work as a technician in the Genetics, Diversity and Ecophysiology of Cereals INRAE lab in Clermont-Ferrand, France.\n\n\nMy work focuses on the characterisation of bread wheat genetic diversity.\n\n\n\nWhat you‚Äôll find on this blog\n\n‚û°Ô∏è Posts about R\n\n\n‚û°Ô∏è Posts about data visualisation"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Books"
  },
  {
    "objectID": "posts/2023-06-23-datardis/index.html",
    "href": "posts/2023-06-23-datardis/index.html",
    "title": "The {datardis} package",
    "section": "",
    "text": "The {datardis} package is available on CRAN:\n\ninstall.packages(\"datardis\")"
  },
  {
    "objectID": "posts/2023-06-23-datardis/index.html#accessing-the-datasets",
    "href": "posts/2023-06-23-datardis/index.html#accessing-the-datasets",
    "title": "The {datardis} package",
    "section": "Accessing the datasets",
    "text": "Accessing the datasets\n\nTo access the datasets, use the following commands:\n\n# Load the package\nlibrary(datardis)\n\n# List of Dr Who episodes\ndrwho_episodes\n\n# List of Dr Who directors\ndrwho_directors\n\n# List of Dr Who writers\ndrwho_writers\n\n# List of Torchwood episodes\ntorchwood_episodes\n\n# List of Torchwood directors\ntorchwood_directors\n\n# List of Torchwood writers\ntorchwood_writers"
  },
  {
    "objectID": "posts/2023-06-23-datardis/index.html#list-of-dr-who-episodes",
    "href": "posts/2023-06-23-datardis/index.html#list-of-dr-who-episodes",
    "title": "The {datardis} package",
    "section": "List of Dr Who episodes",
    "text": "List of Dr Who episodes\n\nTo view the dataset content, use the glimpse() function:\n\n# Load the tidyverse\nlibrary(tidyverse)\n\n# View the dataset content\nglimpse(drwho_episodes)\n\n\n\nRows: 175\nColumns: 12\n$ era             &lt;chr&gt; \"revived\", \"revived\", \"revived\", \"revived\", \"revived\",‚Ä¶\n$ season_number   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ‚Ä¶\n$ serial_title    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ story_number    &lt;chr&gt; \"157\", \"158\", \"159\", \"160a\", \"160b\", \"161\", \"162\", \"16‚Ä¶\n$ episode_number  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, NA, 1, 2, 3‚Ä¶\n$ episode_title   &lt;chr&gt; \"Rose\", \"The End of the World\", \"The Unquiet Dead\", \"A‚Ä¶\n$ type            &lt;chr&gt; \"episode\", \"episode\", \"episode\", \"episode\", \"episode\",‚Ä¶\n$ first_aired     &lt;date&gt; 2005-03-26, 2005-04-02, 2005-04-09, 2005-04-16, 2005-‚Ä¶\n$ production_code &lt;chr&gt; \"1.1\", \"1.2\", \"1.3\", \"1.4\", \"1.5\", \"1.6\", \"1.7\", \"1.8\"‚Ä¶\n$ uk_viewers      &lt;dbl&gt; 10.81, 7.97, 8.86, 7.63, 7.98, 8.63, 8.01, 8.06, 7.11,‚Ä¶\n$ rating          &lt;dbl&gt; 76, 76, 80, 82, 81, 84, 81, 83, 84, 85, 82, 86, 89, 84‚Ä¶\n$ duration        &lt;dbl&gt; 45, 44, 44, 45, 42, 45, 44, 45, 45, 45, 45, 45, 45, 60‚Ä¶"
  },
  {
    "objectID": "posts/2023-06-23-datardis/index.html#who-wrote-the-most-dr-who-episodes",
    "href": "posts/2023-06-23-datardis/index.html#who-wrote-the-most-dr-who-episodes",
    "title": "The {datardis} package",
    "section": "Who wrote the most Dr Who episodes?",
    "text": "Who wrote the most Dr Who episodes?\n\nTo find out who wrote the most Dr Who episodes, use the following code:\n\ndrwho_writers |&gt; \n  count(writer, sort = TRUE) |&gt; \n  head(5)\n\n\n\n# A tibble: 5 √ó 2\n  writer               n\n  &lt;chr&gt;            &lt;int&gt;\n1 Steven Moffat       45\n2 Russell T Davies    31\n3 Chris Chibnall      29\n4 Mark Gatiss          9\n5 Toby Whithouse       7"
  },
  {
    "objectID": "posts/2023-07-31-tt-scurvy/index.html",
    "href": "posts/2023-07-31-tt-scurvy/index.html",
    "title": "#TidyTuesday 2023 - Week 30",
    "section": "",
    "text": "The #TidyTuesday weekly challenge is organised by the R4DS (R for Data Science) Online Learning Community.\nEvery tuesday throughout the year, participants work on a common dataset and share the plots they create.\nThe dataset for this challenge comes from the {medicaldata} R package."
  },
  {
    "objectID": "posts/2023-07-31-tt-scurvy/index.html#text",
    "href": "posts/2023-07-31-tt-scurvy/index.html#text",
    "title": "#TidyTuesday 2023 - Week 30",
    "section": "Text",
    "text": "Text\n\nFirst we create a table for the text to be displayed:\n\n# Create table with text and x/y positions for plot\np_text &lt;- tibble(\n  x = 0,\n  y = c(6, 4:2, 0:-3, -5, -6),\n  text = c(\"In 1757, the cause of scurvy was unknown ...\",\n           \"Aboard HMS Salisbury, the ship's surgeon, James Lind,\",\n           \"tested 6 different treatments in 12 seamen with\",\n           \"symptomatic scurvy.\",\n           \"After six days of therapy, he noted the severity of several\",\n           \"symptoms, including rotting of the gums, skin sores,\",\n           \"weakness of the knees, and lassitude, using a scale ranging\",\n           \"from 0 (none) to 3 (severe).\",\n           \"The figure shows overall improvement of symptoms in the\",\n           \"treated seamen.\"\n           )\n  )\n\nWe then create a first plot with the text:\n\n# Create p0 plot with text\np0 &lt;- ggplot() +\n  geom_text(data = p_text,\n            aes(x = x, y = y, label = text),\n            family = \"Roboto Condensed\", colour = \"white\", hjust = 0, size = 18) +\n  xlim(0, 10) +\n  ylim(-10, 10) +\n  labs(title = \"Curing scurvy\") +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#003049\", color  = \"#003049\"),\n        plot.background = element_rect(fill = \"#003049\", color  = \"#003049\"),\n        plot.title = element_text(family = \"Bangers\", colour = \"white\",\n                                  size = 100, hjust = 0.5, margin = margin(t = 20)))"
  },
  {
    "objectID": "posts/2023-07-31-tt-scurvy/index.html#improvement-scores",
    "href": "posts/2023-07-31-tt-scurvy/index.html#improvement-scores",
    "title": "#TidyTuesday 2023 - Week 30",
    "section": "Improvement scores",
    "text": "Improvement scores\n\nWe represent the overall improvement scores as percentages using coloured bars.\nFirst, we create data points to draw bars ranging from 0 to 100:\n\n# Create data points to draw bars from 0 to 100\np1_bars &lt;- overall_improvement |&gt;\n  # keep treatment column\n  select(treatment) |&gt;\n  # add a row id column named \"y\"\n  rowid_to_column(var = \"y\") |&gt;\n  # repeat each row 101 times (0 to 100)\n  slice(rep(1:n(), each = 101)) |&gt;\n  # add a column with positions to draw bars (for each of the 6 treatments)\n  mutate(x = rep(0:100, times = 6))\n\n\nThen we create data points to represent the actual improvement scores:\n\n# Create data points to draw bars for improvement scores\np1_values &lt;- overall_improvement |&gt;\n  # round improvement score %\n  mutate(improvement_score = round(improvement_score)) |&gt;\n  # select treatment + improvement score columns\n  select(treatment, improvement_score) |&gt;\n  # join p1_bars to get positions for bars from 0 to score\n  left_join(p1_bars) |&gt;\n  # extract max score for each treatment\n  mutate(max_score = max(improvement_score),\n         .by = treatment) |&gt;\n  # remove rows when x &gt; max_score\n  filter(x &lt;= max_score) |&gt;\n  # order columns\n  select(y, treatment, x)\n\n\nFinally, we create the plot:\n\n# Create data points to draw bars for improvement scores\np1 &lt;- ggplot() +\n  geom_segment(data = p1_bars,\n               aes(x = x, xend = x,\n                   y = y - 0.25, yend = y + 0.25,\n                   colour = x),\n               linewidth = 1, alpha = 0.4,\n               show.legend = FALSE) +\n  geom_segment(data = p1_values,\n               aes(x = x, xend = x,\n                   y = y - 0.25, yend = y + 0.25,\n                   colour = x),\n               linewidth = 1,\n               show.legend = FALSE) +\n  geom_text(data = p1_values |&gt; distinct(y, treatment),\n            aes(x = 0, y = y + 0.4, label = treatment),\n            size = 15, colour = \"white\", hjust = 0,\n            family = \"Roboto Condensed\") +\n  scale_colour_gradient2(low = \"#d62828\", mid = \"#f77f00\", high = \"#fcbf49\",\n                         midpoint = 50) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#003049\", color  = \"#003049\"),\n        plot.background = element_rect(fill = \"#003049\", color  = \"#003049\"))"
  },
  {
    "objectID": "posts/2023-07-31-tt-scurvy/index.html#assembling-the-plots",
    "href": "posts/2023-07-31-tt-scurvy/index.html#assembling-the-plots",
    "title": "#TidyTuesday 2023 - Week 30",
    "section": "Assembling the plots",
    "text": "Assembling the plots\n\nWe use the {patchwork} package to assemble the two plots, add some caption text, and export to .png format:\n\n# Assemble the two plots\np &lt;- p0 + p1 +\n  plot_annotation(\n    caption = \"#TidyTuesday 2023 week 30 | Data from {medicaldata} | Jonathan Kitt\",\n    theme = theme(\n      panel.background = element_rect(fill = \"#003049\", color  = \"#003049\"),\n      plot.background = element_rect(fill = \"#003049\", color  = \"#003049\"),\n      plot.caption = element_text(colour = \"white\", hjust = 0.5, size = 30,\n                                  family = \"Roboto Condensed\")\n      )\n    )\n\n# Export to png\nggsave(\"figs/tt_2023_w30_scurvy.png\", p, dpi = 320, width = 12, height = 6)\n\n\nAnd here‚Äôs the result!"
  },
  {
    "objectID": "posts/2023-08-04-tt-us/index.html",
    "href": "posts/2023-08-04-tt-us/index.html",
    "title": "#TidyTuesday 2023 - Week 31",
    "section": "",
    "text": "Introduction\n\nThe #TidyTuesday weekly challenge is organised by the R4DS (R for Data Science) Online Learning Community.\nEvery tuesday throughout the year, participants work on a common dataset and share the plots they create.\nThe dataset for this challenge comes from Wikipedia articles.\n\n\n\nGetting the data\n\nFirst of all, let‚Äôs load the packages we‚Äôll be using :\n\n{tidyverse} to clean the data and create the plots\n{showtext} to change the fonts used\n{rgdal} and {rgeos} to plot the U.S. states as hexagons\n{ggtext} to add colours in the plot title\n\nIf you don‚Äôt have these packages installed, simply use the install.packages() function.\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(rgdal)\nlibrary(rgeos)\nlibrary(ggtext)\n\n\nWe also load the fonts we will use in the plots: Roboto Condensed for the text and Bangers for the title.\n\n# Import the fonts\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\nfont_add_google(\"Bangers\", \"Bangers\")\nshowtext_auto()\n\n\nWe can now download the dataset :\n\n# Download the dataset\nstates &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-08-01/states.csv')\n\n\nWe also download the data to plot U.S. states as hexagons here. We save the GEO JSON file in a raw/ directory.\nWe then load the data:\n\n# Download the dataset\nus_hex &lt;- readOGR(\"raw/us_states_hexgrid.geojson\")\n\nFor a quick overview of the data, we use the glimpse() function from the {dplyr} package:\n\n# Explore the dataset\nglimpse(states)\n\nRows: 50\nColumns: 14\n$ state               &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"Calif‚Ä¶\n$ postal_abbreviation &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"F‚Ä¶\n$ capital_city        &lt;chr&gt; \"Montgomery\", \"Juneau\", \"Phoenix\", \"Little Rock\", ‚Ä¶\n$ largest_city        &lt;chr&gt; \"Huntsville\", \"Anchorage\", \"Phoenix\", \"Little Rock‚Ä¶\n$ admission           &lt;date&gt; 1819-12-14, 1959-01-03, 1912-02-14, 1836-06-15, 1‚Ä¶\n$ population_2020     &lt;dbl&gt; 5024279, 733391, 7151502, 3011524, 39538223, 57737‚Ä¶\n$ total_area_mi2      &lt;dbl&gt; 52420, 665384, 113990, 53179, 163695, 104094, 5543‚Ä¶\n$ total_area_km2      &lt;dbl&gt; 135767, 1723337, 295234, 137732, 423967, 269601, 1‚Ä¶\n$ land_area_mi2       &lt;dbl&gt; 50645, 570641, 113594, 52035, 155779, 103642, 4842‚Ä¶\n$ land_area_km2       &lt;dbl&gt; 131171, 1477953, 294207, 134771, 403466, 268431, 1‚Ä¶\n$ water_area_mi2      &lt;dbl&gt; 1775, 94743, 396, 1143, 7916, 452, 701, 540, 12133‚Ä¶\n$ water_area_km2      &lt;dbl&gt; 4597, 245384, 1026, 2961, 20501, 1170, 1816, 1399,‚Ä¶\n$ n_representatives   &lt;dbl&gt; 7, 1, 9, 4, 52, 8, 5, 1, 28, 14, 2, 2, 17, 9, 4, 4‚Ä¶\n$ demonym             &lt;chr&gt; \"Alabamian\", \"Alaskan\", \"Arizonan\", \"Arkansan\", \"C‚Ä¶\n\n\nThe dataset has 50 observations (rows) and 14 variables (columns).\nEach row represents one U.S. states.\nWe‚Äôre going to represent the ratios between land and water surfaces for each state.\n\n\n\nCleaning the data\n\nWe use the following code to calculate the land/water ratios:\n\n# Land to water area ratios\nland_to_water_ratios &lt;- states |&gt;\n  # calculate land/total and water/total area ratios\n  mutate(land_area_ratio = round(land_area_km2 / total_area_km2, 3),\n         water_area_ratio = round(1 - land_area_ratio, 3)) |&gt;\n  # select columns\n  select(id = postal_abbreviation, state, land_area_ratio, water_area_ratio)\n\n# View first lines of cleaned data\nhead(land_to_water_ratios)\n\n# A tibble: 6 √ó 4\n  id    state      land_area_ratio water_area_ratio\n  &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n1 AL    Alabama              0.966            0.034\n2 AK    Alaska               0.858            0.142\n3 AZ    Arizona              0.997            0.003\n4 AR    Arkansas             0.979            0.021\n5 CA    California           0.952            0.048\n6 CO    Colorado             0.996            0.004\n\n\n\nWe calculate the coordinates for boundaries between land and water surfaces for each hexagon:\n\n# Prepare data to create hex map of land to water ratio per US state\nus_clean &lt;- us_hex |&gt;\n  # transform hex data into table\n  fortify(region = \"iso3166_2\") |&gt;\n  # transform into tibble format\n  as_tibble() |&gt;\n  # select columns\n  select(id, long, lat) |&gt;\n  # join land to water ratios values\n  left_join(land_to_water_ratios) |&gt;\n  # remove District of Columbia\n  filter(id != \"DC\") |&gt;\n  # add column with point order\n  mutate(pt_nb = rep(1:7, times = 50), .before = long) |&gt;\n  # group by state id\n  group_by(id) |&gt;\n  # calculate parameters for each state\n  mutate(\n    # total area for reactangle around hex\n    area_rect = (long[pt_nb == 3] - long[pt_nb == 5]) * (lat[pt_nb == 1] - lat[pt_nb == 4]),\n    # slope of upper hex triangle\n    slope = (lat[pt_nb == 6] - lat[pt_nb == 1]) / (long[pt_nb == 6] - long[pt_nb == 1]),\n    # y coordinate for horizontal border btwn land/water\n    split_y = ((land_area_ratio * area_rect) / (long[pt_nb == 2] - long[pt_nb == 6]) + lat[pt_nb == 4]),\n    # determine type of split : 1 if split line below upper triangle / 2 if not\n    split_type = case_when(split_y &lt;= lat[pt_nb == 2] ~ 1, TRUE ~ 2),\n    # calculate x coordinates for horizontal border btwn land/water\n    split_x1 = case_when(split_type == 1 ~ min(long),\n                         split_type == 2 ~ long[pt_nb == 6] + ((split_y - lat[pt_nb == 6]) / slope)),\n    split_x2 = case_when(split_type == 1 ~ max(long),\n                         split_type == 2 ~ long[pt_nb == 1] + (long[pt_nb == 1] - split_x1))\n    )\n\n# Subset us_clean data for type 1 split\nus_clean_type1 &lt;- us_clean |&gt;\n  # filter data\n  filter(split_type == 1) |&gt;\n  # create new columns to keep points needed for plot\n  mutate(pt1_x = long[pt_nb == 1], pt1_y = lat[pt_nb == 1],\n         pt2_x = long[pt_nb == 2], pt2_y = lat[pt_nb == 2],\n         pt3_x = unique(split_x2), pt3_y = unique(split_y),\n         pt4_x = unique(split_x1), pt4_y = unique(split_y),\n         pt5_x = long[pt_nb == 6], pt5_y = lat[pt_nb == 6],\n         pt6_x = pt1_x, pt6_y = pt1_y) |&gt;\n  # select columns\n  select(id, pt1_x:pt6_y) |&gt;\n  # ungroup data\n  ungroup() |&gt;\n  # pivot to long format\n  pivot_longer(cols = -id, names_to = \"pt\", values_to = \"value\") |&gt;\n  # separate \"pt\" column\n  separate(col = pt, into = c(\"pt_nb\", \"xy\"), sep = \"_\") |&gt;\n  # remove \"pt\" string from pt_nb column\n  mutate(pt_nb = str_remove_all(pt_nb, \"pt\")) |&gt;\n  # keep distinct rows\n  distinct() |&gt;\n  # pivot to wide format\n  pivot_wider(id_cols = id:pt_nb, names_from = \"xy\", values_from = \"value\")\n\n# Subset us_clean data for type 2 split\nus_clean_type2 &lt;- us_clean |&gt;\n  # filter data\n  filter(split_type == 2) |&gt;\n  # create new columns to keep points needed for plot\n  mutate(pt1_x = long[pt_nb == 1], pt1_y = lat[pt_nb == 1],\n         pt2_x = unique(split_x2), pt2_y = unique(split_y),\n         pt3_x = unique(split_x1), pt3_y = unique(split_y),\n         pt4_x = pt1_x, pt4_y = pt1_y) |&gt;\n  # select columns\n  select(id, pt1_x:pt4_y) |&gt;\n  # ungroup data\n  ungroup() |&gt;\n  # pivot to long format\n  pivot_longer(cols = -id, names_to = \"pt\", values_to = \"value\") |&gt;\n  # separate \"pt\" column\n  separate(col = pt, into = c(\"pt_nb\", \"xy\"), sep = \"_\") |&gt;\n  # remove \"pt\" string from pt_nb column\n  mutate(pt_nb = str_remove_all(pt_nb, \"pt\")) |&gt;\n  # keep distinct rows\n  distinct() |&gt;\n  # pivot to wide format\n  pivot_wider(id_cols = id:pt_nb, names_from = \"xy\", values_from = \"value\")\n\n\nWe calculate the coordinates of each hexagon‚Äôs centre to add text labels:\n\n# Calculate centres of polygons to plot state labels\ncentres &lt;- gCentroid(us_hex, byid = TRUE) |&gt;\n  as_tibble()\n\nlabels &lt;- us_hex@data$iso3166_2\n\nhex_labels &lt;- tibble(id = labels,\n                     centres) |&gt;\n  filter(id != \"DC\")\n\n# Clean global environment\nrm(centres, labels, land_to_water_ratios, states, us_hex)\n\n\n\n\nCreating the plot\n\n\np &lt;- ggplot() +\n  geom_polygon(data = us_clean, aes(x = long, y = lat, group = id),\n               colour = NA, fill = \"#48bf91\") +\n  geom_polygon(data = us_clean_type1, aes(x = x, y = y, group = id),\n               colour = NA, fill = \"#0076be\") +\n  geom_polygon(data = us_clean_type2, aes(x = x, y = y, group = id),\n               colour = NA, fill = \"#0076be\") +\n  geom_polygon(data = us_clean, aes(x = long, y = lat, group = id),\n               colour = \"white\", fill = NA, linewidth = 0.5) +\n  geom_text(data = hex_labels, aes(x = x, y = y, label = id),\n            family = \"Roboto Condensed\", colour = \"black\", size = 16) +\n  coord_map() +\n  labs(title = \"&lt;span style='color:#48bf91;'&gt;Land&lt;/span&gt; to &lt;span style='color:#0076be;'&gt;water&lt;/span&gt; surface ratios in the U.S.\",\n       caption = \"#TidyTuesday 2023 week 31 | Data from Wikipedia | Jonathan Kitt\") +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"black\", colour = NA),\n        plot.background = element_rect(fill = \"black\", colour = NA),\n        plot.title = element_markdown(family = \"Bangers\", size = 90,\n                                      hjust = 0.5, colour = \"white\",\n                                      margin = margin(t = 20)),\n        plot.caption = element_text(family = \"Roboto Condensed\", colour = \"white\", size = 30,\n                                    hjust = 0.5, \n                                    margin = margin(b = 5)))\n\nggsave(\"figs/tt_2023_w31_us.png\", p, dpi = 320, width = 12, height = 6)\n\n\nAnd here‚Äôs the result!"
  }
]